<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mx's Blog</title><link href="http://x-wei.github.io" rel="alternate"></link><link href="http://x-wei.github.io/feeds/notes.atom.xml" rel="self"></link><id>http://x-wei.github.io</id><updated>2015-06-23T00:00:00+02:00</updated><entry><title>[Spark MOOC note] Lec8. Exploratory Data Anakysis and Machine Learning</title><link href="http://x-wei.github.io/sparkmooc_note_lec8.html" rel="alternate"></link><updated>2015-06-23T00:00:00+02:00</updated><author><name>mx</name></author><id>http://x-wei.github.io/sparkmooc_note_lec8.html</id><summary type="html">&lt;h2&gt;STATISTICS, BUSINESS QUESTIONS, AND LEARNING TECHNIQUES&lt;/h2&gt;
&lt;p&gt;2 different kinds of statistics: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;descriptive statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ex. median — describes data, &lt;em&gt;but cannot generalize beyong that&lt;/em&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inferential statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ex. &lt;em&gt;t-testing — inferences beyond the data&lt;/em&gt;
techniques leveraged for machine learning and prediction&lt;/p&gt;
&lt;p&gt;supervised learning (clf, reg), unsupervised learning (clustering, dim-reduction)
 → UL often used in a larger SL pb (ex. &lt;em&gt;auto-encoder&lt;/em&gt;)
&lt;img alt="" src="sparkmooc_note_lec8/pasted_image.png" /&gt;&lt;/p&gt;
&lt;h2&gt;EXPLORATORY DATA ANALYSIS&lt;/h2&gt;
&lt;p&gt;5-number summary:&lt;/p&gt;
&lt;p&gt;The five-number summary is a descriptive statistic that provides information about a set of observations. It consists of the five most important sample percentiles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The sample minimum (smallest observation)&lt;/li&gt;
&lt;li&gt;The lower quartile or first quartile&lt;/li&gt;
&lt;li&gt;The median (middle value)&lt;/li&gt;
&lt;li&gt;The upper quartile or third quartile&lt;/li&gt;
&lt;li&gt;The sample maximum (largest observation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec8/pasted_image001.png" /&gt;&lt;/p&gt;
&lt;p&gt;→ box plot: 
&lt;img alt="" src="sparkmooc_note_lec8/pasted_image004.png" /&gt;&lt;/p&gt;
&lt;h2&gt;THE R LANGUAGE AND NORMAL DISTRIBUTIONS&lt;/h2&gt;
&lt;p&gt;R: intractive exploration and visulization of data + statistical models and distributions + CRAN&lt;/p&gt;
&lt;p&gt;Central Limit Th: sum/mean of n iid random variables 
many statistical test assume data to be normally distributed&lt;/p&gt;
&lt;h2&gt;DISTRIBUTIONS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;poissons distribution: accurrence freq&lt;/li&gt;
&lt;li&gt;exponential distribution: interval between 2 (poissons) events&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Zipf/Pareto/Yule distributions&lt;/em&gt;: frequencies of different terms in a document, or web site visits&lt;/li&gt;
&lt;li&gt;binomial/multinomial distribution: nb of count of events&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;RHINE PARADOX&lt;/h2&gt;
&lt;h2&gt;SPARK'S MACHINE LEARNING TOOLKIT&lt;/h2&gt;
&lt;p&gt;mllib: scalable, distributed ML library, &lt;em&gt;sklearn-like&lt;/em&gt; ML toolkit
&lt;a href="https://spark.apache.org/docs/latest/mllib-guide.html"&gt;https://spark.apache.org/docs/latest/mllib-guide.html&lt;/a&gt;
lab: &lt;em&gt;collaborative filtering — &lt;/em&gt;matrix factorisation
&lt;img alt="" src="sparkmooc_note_lec8/pasted_image005.png" /&gt;
⇒ &lt;em&gt;alternating&lt;/em&gt; least square(ALS): 
&lt;img alt="" src="sparkmooc_note_lec8/pasted_image006.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;trouble with summary stats&lt;/strong&gt;: &lt;em&gt;Anscombe's Quartet&lt;/em&gt;
→ have same statistics property
&lt;img alt="" src="sparkmooc_note_lec8/pasted_image002.png" /&gt;
→ quite different in fact: 
&lt;img alt="" src="sparkmooc_note_lec8/pasted_image003.png" /&gt;
&lt;strong&gt;Takeaways&lt;/strong&gt;:&lt;br /&gt;
&lt;em&gt;•  Important to look at data graphically before analyzing it   &lt;/em&gt;
&lt;em&gt;•  Basic statistics properties often fail to capture real-world complexities&lt;/em&gt;  &lt;/p&gt;
&lt;h2&gt;Lab3. Text Analysis and Entity Resolution&lt;/h2&gt;
&lt;p&gt;Entity Resolution (ER) refers to the task of finding records in a data set that refer to the same entity across different data sources (e.g., data files, books, websites, databases). ER is necessary when joining data sets based on entities that may or may not share a common identifier (e.g., database key, URI, National identification number), as may be the case due to differences in record shape, storage location, and/or curator style or preference. A data set that has undergone ER may be referred to as being cross-linked.&lt;/p&gt;
&lt;p&gt;The file format of an Amazon line is:
"id","title","description","manufacturer","price"
The file format of a Google line is:
"id","name","description","manufacturer","price"&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;re.split&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;re.split()有个很讨厌的地方: 字符串以句号等结尾时, 最后总是会出现一个空字符串:
    &amp;gt;&amp;gt;&amp;gt; re.split('\W+', 'Words, words, words.')
    ['Words', 'words', 'words', '']
解决办法就是用个filter:　
&lt;code&gt;filter(None,re.split(split_regex, string.lower()) )&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tfidf&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TF rewards tokens that appear many times in the same document. It is computed as the frequency of a token in a document. IDF rewards tokens that are rare overall in a dataset. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cosine similarity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The metric of string distance we will use is called cosine similarity. We will treat each document as a vector in some high dimensional space. Then, to compare two documents we compute the cosine of the angle between their two document vectors. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;flatMap&lt;/code&gt;: 一行变多行, 别忘了...&lt;/li&gt;
&lt;li&gt;broadcast variable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;we define the broadcast variable in the driver and then we can refer to it in each worker. Spark saves the broadcast variable at each worker, so it is only sent once.
声明广播变量的办法也很简单, 只要:
 &lt;code&gt;idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)&lt;/code&gt;
然后用的时候要改成&lt;code&gt;xx.value&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EXCEPT语句&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;找了一下没发现spark有SQL的EXCEPT语句(就是和join相反), 于是只好这么写了:
    nonDupsRDD = (sims
                  .leftOuterJoin(goldStandard)
                 .filter(lambda x: x[1][1]==None)
                 .map(lambda x:(x[0],x[1][0])))
用leftouterjoin 然后再只保留为None的那些... 应该不是最佳写法吧...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;complexity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用的ER办法(cosine similarity)的复杂度是O2 太高了...
⇒ An &lt;strong&gt;inverted index&lt;/strong&gt; is a data structure that will allow us to avoid making quadratically many token comparisons. It maps each token in the dataset to &lt;em&gt;the list of documents that contain the token&lt;/em&gt;. So, instead of comparing, record by record, each token to every other token to see if they match, we will use inverted indices to &lt;em&gt;look up records(documents) that match on a particular token&lt;/em&gt;.
这种操作的基础是: 有很多向量的support是完全不重合的 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.&lt;code&gt;collectAsMap()&lt;/code&gt;: 把pair rdd变为map&lt;/li&gt;
&lt;li&gt;groupByKey(): 这个也用上了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;lab4前后做了四个小时 不过很有意思... 第五部分出现out of memory error就没办法了...&lt;/p&gt;</summary><category term="spark"></category></entry><entry><title>[Spark MOOC note] Lec7. Data Quality</title><link href="http://x-wei.github.io/sparkmooc_note_lec7.html" rel="alternate"></link><updated>2015-06-22T00:00:00+02:00</updated><author><name>mx</name></author><id>http://x-wei.github.io/sparkmooc_note_lec7.html</id><summary type="html">&lt;h2&gt;DATA CLEANING&lt;/h2&gt;
&lt;p&gt;ex. 
deal with missing data, entity resolution, unit mismatch, ... &lt;/p&gt;
&lt;p&gt;deal with non-ideal samples ⇒ tradeoff between simplicity and accuracy. &lt;/p&gt;
&lt;h2&gt;DATA QUALITY PROBLEMS&lt;/h2&gt;
&lt;p&gt;data quality problems: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conversions in complex pipelines can mess up data &lt;/li&gt;
&lt;li&gt;Combining multiple datasets can result in errrors&lt;/li&gt;
&lt;li&gt;Data degrades in accuracy or loses value over time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还提供了一些工具帮助cleaning data: &lt;a href="http://vis.stanford.edu/wrangler/"&gt;http://vis.stanford.edu/wrangler/&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;EXAMPLE: AGES OF STUDENTS IN THIS COURSE&lt;/h2&gt;
&lt;p&gt;(students' ages are self-reported...)&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec7/pasted_image.png" /&gt;&lt;/p&gt;
&lt;h2&gt;DATA CLEANING MAKES EVERYTHING OKAY?&lt;/h2&gt;
&lt;p&gt;ex. the appearance of a hole in the ozone layer. &lt;/p&gt;
&lt;h2&gt;DIRTY DATA PROBLEMS&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec7/pasted_image001.png" /&gt;&lt;/p&gt;
&lt;p&gt;Data Quality Continuum:&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec7/pasted_image002.png" /&gt;&lt;/p&gt;
&lt;h2&gt;DATA GATHERING&lt;/h2&gt;
&lt;p&gt;solutions in the data gathering stage: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;re-emptive (先发制人) &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;integrity checks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;retrospective&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;duplicate removal&lt;/p&gt;
&lt;h2&gt;DATA DELIVERY&lt;/h2&gt;
&lt;p&gt;solutions: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec7/pasted_image003.png" /&gt;&lt;/p&gt;
&lt;h2&gt;DATA STORAGE&lt;/h2&gt;
&lt;p&gt;physical pb: storage is cheap → use data redundancy 
logical pb: poor metadata, etc&lt;/p&gt;
&lt;p&gt;⇒ solutions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;publish &lt;em&gt;data specifications&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;data mining tools&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DATA RETRIEVAL&lt;/h2&gt;
&lt;p&gt;...总之就是各种方面都会引起data quality pb... &lt;/p&gt;
&lt;h2&gt;DATA QUALITY CONSTRAINTS&lt;/h2&gt;
&lt;p&gt;static constraints: 
ex. nulls not allowed, field domains&lt;/p&gt;
&lt;p&gt;data constraints follow a 80-20 rule: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec7/pasted_image004.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data quality metrics&lt;/strong&gt;: ...
ex. in lab2, examine log lines that are not correctly parsed.&lt;/p&gt;
&lt;h2&gt;TECHNICAL APPROACHES TO DATA QUALITY&lt;/h2&gt;
&lt;p&gt;ex. entity resolution in lab3&lt;/p&gt;
&lt;h2&gt;EXAMPLE: DEDUP/CLEANING&lt;/h2&gt;
&lt;p&gt;bing shopping被黑了
convert to &lt;em&gt;canonical form &lt;/em&gt;(ex. mailing address)&lt;/p&gt;</summary><category term="spark"></category></entry><entry><title>[Spark MOOC note] Lec5. Semi-structured Data</title><link href="http://x-wei.github.io/sparkmooc_note_lec5.html" rel="alternate"></link><updated>2015-06-17T00:00:00+02:00</updated><author><name>mx</name></author><id>http://x-wei.github.io/sparkmooc_note_lec5.html</id><summary type="html">&lt;h2&gt;KEY DATA MANAGEMENT CONCEPTS&lt;/h2&gt;
&lt;p&gt;data model: collection of concepts for describing data
schema: a description of a particular collection of data using a given data model&lt;/p&gt;
&lt;p&gt;structure spectrum: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image.png" /&gt;
semi-structured data: apply schema &lt;strong&gt;after&lt;/strong&gt; creating data. &lt;/p&gt;
&lt;h2&gt;FILES&lt;/h2&gt;
&lt;p&gt;files: named collection of bytes, in hierarchical namespace (but: In a Content-Addressable Storage system files are stored, arranged, and accessed based on their content or metadata, not in hierarchy)&lt;/p&gt;
&lt;h2&gt;SEMI-STRUCTURED TABULAR DATA&lt;/h2&gt;
&lt;p&gt;table: a collection of rows and columns, each row has an &lt;em&gt;index&lt;/em&gt;, each column has a &lt;em&gt;name&lt;/em&gt;. 
cell: by a pair (row, col), values can be missing, types are &lt;em&gt;inffered&lt;/em&gt; from content&lt;/p&gt;
&lt;p&gt;CSV:&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image002.png" /&gt;&lt;/p&gt;
&lt;p&gt;PDB:(filed name can be repeated on multuple lines)  &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image001.png" /&gt; &lt;/p&gt;
&lt;h2&gt;CHALLENGES WITH TABULAR DATA&lt;/h2&gt;
&lt;p&gt;challenges: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image003.png" /&gt;&lt;/p&gt;
&lt;p&gt;challenges for tabular data &lt;em&gt;from multiple source&lt;/em&gt;: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image004.png" /&gt;&lt;/p&gt;
&lt;p&gt;challenges for tabular data &lt;em&gt;from sensors&lt;/em&gt;: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image005.png" /&gt;&lt;/p&gt;
&lt;h2&gt;PANDAS AND SEMI-STRUCTURED DATA IN PYSPARK&lt;/h2&gt;
&lt;p&gt;pandas &lt;code&gt;DataFrame&lt;/code&gt;: represented as python dict (colname → series)
pandas &lt;code&gt;Series&lt;/code&gt;: 1D labeled array capable of holding any data type&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;spark DataFrame&lt;/strong&gt;: &lt;em&gt;Distributed&lt;/em&gt; collection of data organized into named columns. 
types of columns are inferred from values. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec5/pasted_image006.png" /&gt;&lt;/p&gt;
&lt;p&gt;Using dataframes can be 5 times faster than using RDDs: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image007.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SEMI-STRUCTURED LOG FILES&lt;/h2&gt;
&lt;p&gt;ex. Apache web server log format&lt;/p&gt;
&lt;h2&gt;EXPLORING A WEB SERVER ACCESS LOG&lt;/h2&gt;
&lt;p&gt;NASA http server access log&lt;br /&gt;
&lt;a href="http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html"&gt;http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;DATA MINING LOG FILES&lt;/h2&gt;
&lt;p&gt;Data mining log files is a data exploration process that often involves searching through the data for unusual events, a task that can be done using dashboards for visualizing anomalies. The data being analyzed usually includes machine resource usage data and application queue information.&lt;/p&gt;
&lt;h2&gt;FILE PERFORMANCE&lt;/h2&gt;
&lt;p&gt;binary/text performance benchmark:&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image008.png" /&gt;&lt;br /&gt;
⇒&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;read and write times are comparable &lt;/li&gt;
&lt;li&gt;binary files are mach faster than palin text files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;compression performance benchmark:&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image009.png" /&gt;&lt;br /&gt;
⇒ &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;write times are much larger than read times &lt;/li&gt;
&lt;li&gt;small range of compressed file size&lt;/li&gt;
&lt;li&gt;binary still much faster than text &lt;/li&gt;
&lt;li&gt;LZ4 compression ~= raw IO speed&lt;/li&gt;
&lt;/ul&gt;</summary><category term="spark"></category></entry><entry><title>[Spark MOOC note] Lec6. Structured Data</title><link href="http://x-wei.github.io/sparkmooc_note_lec6.html" rel="alternate"></link><updated>2015-06-17T00:00:00+02:00</updated><author><name>mx</name></author><id>http://x-wei.github.io/sparkmooc_note_lec6.html</id><summary type="html">&lt;h2&gt;RELATIONAL DATABASE&lt;/h2&gt;
&lt;p&gt;review: key data management concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data model &lt;/li&gt;
&lt;li&gt;schema&lt;/li&gt;
&lt;li&gt;&lt;em&gt;relational data model&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;structured data: have a specific schema to start with&lt;/p&gt;
&lt;p&gt;relationl database: a set of relations.
2 parts to a Relation: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;schema: name of relation, name and type of columns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec6//pasted_image.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instance: &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;any data at given time 
(&lt;em&gt;cardinality&lt;/em&gt;:=nb of rows, &lt;em&gt;degree&lt;/em&gt;:=nb of fields)&lt;/p&gt;
&lt;h2&gt;LARGE DATABASES&lt;/h2&gt;
&lt;h2&gt;RELATIONAL DATABASE EXAMPLE AND DISCUSSION&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec6//pasted_image001.png" /&gt; &lt;br /&gt;
cardinality=3
degree=5&lt;/p&gt;
&lt;p&gt;advantages of Relational Databases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;well-def structure&lt;/li&gt;
&lt;li&gt;maintain indices for high performance&lt;/li&gt;
&lt;li&gt;consistancy maintained by transactions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;disadvantages: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;limited, rigid structure&lt;/li&gt;
&lt;li&gt;most disk space taken by large indices&lt;/li&gt;
&lt;li&gt;transactions are slow&lt;/li&gt;
&lt;li&gt;poor support for &lt;em&gt;sparse data&lt;/em&gt;(which is common)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;STRUCTURED QUERY LANGUAGE (SQL)&lt;/h2&gt;
&lt;p&gt;supported by &lt;strong&gt;DataFrame&lt;/strong&gt; of pyspark &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec6//pasted_image002.png" /&gt;&lt;/p&gt;
&lt;h2&gt;JOINS IN SQL&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec6//pasted_image003.png" /&gt;
cross join: carteian product&lt;/p&gt;
&lt;h2&gt;EXPLICIT SQL JOINS&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec6//pasted_image004.png" /&gt;
explicit version is preferred&lt;/p&gt;
&lt;h2&gt;TYPES OF SQL JOINS&lt;/h2&gt;
&lt;p&gt;⇒ controls how &lt;em&gt;unmatched&lt;/em&gt; keys are handled&lt;/p&gt;
&lt;p&gt;LEFT OUTER JOIN: 
keys appearring in left table but not in right table will be included with NULL as value&lt;/p&gt;
&lt;h2&gt;JOINS IN SPARK&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;for spark DataFrame: support inner/left outer/semi-join&lt;/li&gt;
&lt;li&gt;for &lt;em&gt;pair RDDs&lt;/em&gt;: support inner join(), leftOuterJoin(), fullOuterJoin()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;join ex:
&lt;img alt="" src="sparkmooc_note_lec6//pasted_image005.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec6//pasted_image006.png" /&gt;&lt;/p&gt;
&lt;p&gt;outerjoin ex:
&lt;img alt="" src="sparkmooc_note_lec6//pasted_image007.png" /&gt;&lt;/p&gt;
&lt;p&gt;fullouterjoin ex:
&lt;img alt="" src="sparkmooc_note_lec6//pasted_image008.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Lab 2 - Web Server Log Analysis with Apache Spark&lt;/h2&gt;
&lt;p&gt;Apache Common Log Format (CLF):&lt;br /&gt;
&lt;code&gt;127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] "GET /images/launch-logo.gif HTTP/1.0" 200 1839&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Row(
host          = match.group(1),
client_identd = match.group(2),
user_id       = match.group(3),
date_time     = parse_apache_time(match.group(4)),
method        = match.group(5),
endpoint      = match.group(6),
protocol      = match.group(7),
response_code = int(match.group(8)),
content_size  = size 
)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;distinctByKey&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个pair RDD按照key来distinct不知道有没有distinctByKey之类的东西, 只好写成这样, 不知是不是对的: 
&lt;code&gt;dayHostCount = dayGroupedHosts.map(lambda group : (group[0], len(set(group[1])) ) )&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;...总体来说很有意思的一个lab...&lt;/p&gt;</summary><category term="spark"></category></entry><entry><title>[Spark MOOC note] Lec4. Spark Essentials</title><link href="http://x-wei.github.io/sparkmooc_note_lec4.html" rel="alternate"></link><updated>2015-06-16T00:00:00+02:00</updated><author><name>mx</name></author><id>http://x-wei.github.io/sparkmooc_note_lec4.html</id><summary type="html">&lt;h2&gt;PYTHON SPARK (PYSPARK)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;a spark prog has 2 programs:&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dirver program: runs on driver machine&lt;/li&gt;
&lt;li&gt;worker program: runs on local threads or cluster nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;a spark prog first creates a &lt;strong&gt;SparkContext object:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tells how and where to access a cluster&lt;/li&gt;
&lt;li&gt;shell will automatically create &lt;strong&gt;the sc varible&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;in iPython: use constructor to create a &lt;code&gt;SparkContext&lt;/code&gt; obj&lt;/li&gt;
&lt;li&gt;⇒ use this SparkContext obj to create RDDs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Master:&lt;br /&gt;
The &lt;code&gt;master&lt;/code&gt; parameter (for a SparkContext) determines which type and size of cluster to use
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image.png" /&gt;&lt;/p&gt;
&lt;h2&gt;RDDs&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Resilient Distributed Dataset&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;immutable once created&lt;/li&gt;
&lt;li&gt;spark tracks linege information to compute lost data efficiently&lt;/li&gt;
&lt;li&gt;operations on collections of elements in parallel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;to create RDDs&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paralizing existing python collections&lt;/li&gt;
&lt;li&gt;transforming existing RDDs&lt;/li&gt;
&lt;li&gt;from files&lt;/li&gt;
&lt;li&gt;can specify the number of partitions for an RDD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image002.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 types of operations on RDD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tranformation: lazy, &lt;em&gt;executed only one action runs on it&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;action&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Working with RDD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create an RDD&lt;/li&gt;
&lt;li&gt;apply transformations to that RDD (ex. map, filter)&lt;/li&gt;
&lt;li&gt;apply actions on RDD (collect, count)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ex code:  &lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;rDD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paralize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;distFile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;textFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;readme.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;// elements are lines in the file&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;SPARK TRANSFORMATIONS&lt;/h2&gt;
&lt;p&gt;to create new dataset from existing one (lazy)&lt;/p&gt;
&lt;p&gt;examples of transformations: &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image003.png" /&gt;&lt;/p&gt;
&lt;h2&gt;PYTHON LAMBDA FUNCTIONS&lt;/h2&gt;
&lt;p&gt;single expression&lt;/p&gt;
&lt;h2&gt;TRANSFORMATIONS&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image004.png" /&gt;&lt;/p&gt;
&lt;p&gt;⇒ spark truns the function litral into a cloture, balck code runs in driver, green code in workers&lt;/p&gt;
&lt;h2&gt;SPARK ACTIONS&lt;/h2&gt;
&lt;p&gt;cause spark to execute recipe to transform source. 
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image006.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SPARK PROGRAMMING MODEL&lt;/h2&gt;
&lt;h2&gt;CACHING RDDS&lt;/h2&gt;
&lt;p&gt;to avoid having to reload data: &lt;code&gt;rdd.cache()&lt;/code&gt;⇒ read from memory instead of disk&lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image007.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SPARK PROGRAM LIFECYCLE&lt;/h2&gt;
&lt;p&gt;create/paralise ⇒ transform ⇒ [cache] ⇒ action&lt;/p&gt;
&lt;h2&gt;SPARK KEY-VALUE RDDS&lt;/h2&gt;
&lt;p&gt;each element of a &lt;em&gt;pair RDD&lt;/em&gt; is a pair tuple&lt;/p&gt;
&lt;p&gt;key-value transformations: &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image008.png" /&gt;&lt;/p&gt;
&lt;p&gt;ex:&lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image009.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image010.png" /&gt;&lt;/p&gt;
&lt;p&gt;careful using &lt;code&gt;groupByKey&lt;/code&gt;: create lots of data traffic and iterables at works&lt;/p&gt;
&lt;h2&gt;PYSPARK CLOSURES&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;one closure per worker is sent &lt;em&gt;with every task&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;no communication between workers&lt;/li&gt;
&lt;li&gt;changes to global vars will not effect driver / other workers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ pbs: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inefficient to send large data to each job&lt;/li&gt;
&lt;li&gt;one-way: driver → worker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;pyspark shared vaiables&lt;/strong&gt;: 
2 types: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Broadcase variables&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;send large, read-only variables to all workers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accumulators&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;aggregate values from worker to drivers &lt;/li&gt;
&lt;li&gt;only driver can access its value&lt;/li&gt;
&lt;li&gt;for workers the accumulators are write-only&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;SPARK BROADCAST VARIABLES&lt;/h2&gt;
&lt;p&gt;ex. give every worker a large dataset &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image011.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SPARK ACCUMULATORS&lt;/h2&gt;
&lt;p&gt;can only be "add" to by associative operation &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image012.png" /&gt;&lt;/p&gt;
&lt;p&gt;careful to use accumulators in transformations: &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image013.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Lab1&lt;/h2&gt;
&lt;p&gt;VB更新以后虚拟机打不开了, 解决办法在: &lt;br /&gt;
&lt;a href="http://bbs.deepin.org/forum.php?mod=viewthread&amp;amp;tid=26001"&gt;http://bbs.deepin.org/forum.php?mod=viewthread&amp;amp;tid=26001&lt;/a&gt;&lt;/p&gt;</summary><category term="spark"></category></entry></feed>