<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mx's Blog</title><link href="http://x-wei.github.io" rel="alternate"></link><link href="http://x-wei.github.io/feeds/notes.atom.xml" rel="self"></link><id>http://x-wei.github.io</id><updated>2015-06-17T00:00:00+02:00</updated><entry><title>[Spark MOOC note] Lec5. Semi-structured Data</title><link href="http://x-wei.github.io/sparkmooc_note_lec5.html" rel="alternate"></link><updated>2015-06-17T00:00:00+02:00</updated><author><name>mx</name></author><id>http://x-wei.github.io/sparkmooc_note_lec5.html</id><summary type="html">&lt;h2&gt;KEY DATA MANAGEMENT CONCEPTS&lt;/h2&gt;
&lt;p&gt;data model: collection of concepts for describing data
schema: a description of a particular collection of data using a given data model&lt;/p&gt;
&lt;p&gt;structure spectrum: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image.png" /&gt;
semi-structured data: apply schema &lt;strong&gt;after&lt;/strong&gt; creating data. &lt;/p&gt;
&lt;h2&gt;FILES&lt;/h2&gt;
&lt;p&gt;files: named collection of bytes, in hierarchical namespace (but: In a Content-Addressable Storage system files are stored, arranged, and accessed based on their content or metadata, not in hierarchy)&lt;/p&gt;
&lt;h2&gt;SEMI-STRUCTURED TABULAR DATA&lt;/h2&gt;
&lt;p&gt;table: a collection of rows and columns, each row has an &lt;em&gt;index&lt;/em&gt;, each column has a &lt;em&gt;name&lt;/em&gt;. 
cell: by a pair (row, col), values can be missing, types are &lt;em&gt;inffered&lt;/em&gt; from content&lt;/p&gt;
&lt;p&gt;CSV:&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image002.png" /&gt;&lt;/p&gt;
&lt;p&gt;PDB:(filed name can be repeated on multuple lines)  &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image001.png" /&gt; &lt;/p&gt;
&lt;h2&gt;CHALLENGES WITH TABULAR DATA&lt;/h2&gt;
&lt;p&gt;challenges: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image003.png" /&gt;&lt;/p&gt;
&lt;p&gt;challenges for tabular data &lt;em&gt;from multiple source&lt;/em&gt;: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image004.png" /&gt;&lt;/p&gt;
&lt;p&gt;challenges for tabular data &lt;em&gt;from sensors&lt;/em&gt;: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image005.png" /&gt;&lt;/p&gt;
&lt;h2&gt;PANDAS AND SEMI-STRUCTURED DATA IN PYSPARK&lt;/h2&gt;
&lt;p&gt;pandas &lt;code&gt;DataFrame&lt;/code&gt;: represented as python dict (colname → series)
pandas &lt;code&gt;Series&lt;/code&gt;: 1D labeled array capable of holding any data type&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;spark DataFrame&lt;/strong&gt;: &lt;em&gt;Distributed&lt;/em&gt; collection of data organized into named columns. 
types of columns are inferred from values. &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="sparkmooc_note_lec5/pasted_image006.png" /&gt;&lt;/p&gt;
&lt;p&gt;Using dataframes can be 5 times faster than using RDDs: &lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image007.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SEMI-STRUCTURED LOG FILES&lt;/h2&gt;
&lt;p&gt;ex. Apache web server log format&lt;/p&gt;
&lt;h2&gt;EXPLORING A WEB SERVER ACCESS LOG&lt;/h2&gt;
&lt;p&gt;NASA http server access log&lt;br /&gt;
&lt;a href="http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html"&gt;http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;DATA MINING LOG FILES&lt;/h2&gt;
&lt;p&gt;Data mining log files is a data exploration process that often involves searching through the data for unusual events, a task that can be done using dashboards for visualizing anomalies. The data being analyzed usually includes machine resource usage data and application queue information.&lt;/p&gt;
&lt;h2&gt;FILE PERFORMANCE&lt;/h2&gt;
&lt;p&gt;binary/text performance benchmark:&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image008.png" /&gt;&lt;br /&gt;
⇒&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;read and write times are comparable &lt;/li&gt;
&lt;li&gt;binary files are mach faster than palin text files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;compression performance benchmark:&lt;br /&gt;
&lt;img alt="" src="sparkmooc_note_lec5/pasted_image009.png" /&gt;&lt;br /&gt;
⇒ &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;write times are much larger than read times &lt;/li&gt;
&lt;li&gt;small range of compressed file size&lt;/li&gt;
&lt;li&gt;binary still much faster than text &lt;/li&gt;
&lt;li&gt;LZ4 compression ~= raw IO speed&lt;/li&gt;
&lt;/ul&gt;</summary><category term="spark"></category></entry><entry><title>[Spark MOOC note] Lec4. Spark Essentials</title><link href="http://x-wei.github.io/sparkmooc_note_lec4.html" rel="alternate"></link><updated>2015-06-16T00:00:00+02:00</updated><author><name>mx</name></author><id>http://x-wei.github.io/sparkmooc_note_lec4.html</id><summary type="html">&lt;h2&gt;PYTHON SPARK (PYSPARK)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;a spark prog has 2 programs:&lt;/strong&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dirver program: runs on driver machine&lt;/li&gt;
&lt;li&gt;worker program: runs on local threads or cluster nodes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;a spark prog first creates a &lt;strong&gt;SparkContext object:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tells how and where to access a cluster&lt;/li&gt;
&lt;li&gt;shell will automatically create &lt;strong&gt;the sc varible&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;in iPython: use constructor to create a &lt;code&gt;SparkContext&lt;/code&gt; obj&lt;/li&gt;
&lt;li&gt;⇒ use this SparkContext obj to create RDDs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Master:&lt;br /&gt;
The &lt;code&gt;master&lt;/code&gt; parameter (for a SparkContext) determines which type and size of cluster to use
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image.png" /&gt;&lt;/p&gt;
&lt;h2&gt;RDDs&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Resilient Distributed Dataset&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;immutable once created&lt;/li&gt;
&lt;li&gt;spark tracks linege information to compute lost data efficiently&lt;/li&gt;
&lt;li&gt;operations on collections of elements in parallel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;to create RDDs&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paralizing existing python collections&lt;/li&gt;
&lt;li&gt;transforming existing RDDs&lt;/li&gt;
&lt;li&gt;from files&lt;/li&gt;
&lt;li&gt;can specify the number of partitions for an RDD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image002.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 types of operations on RDD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tranformation: lazy, &lt;em&gt;executed only one action runs on it&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;action&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Working with RDD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create an RDD&lt;/li&gt;
&lt;li&gt;apply transformations to that RDD (ex. map, filter)&lt;/li&gt;
&lt;li&gt;apply actions on RDD (collect, count)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ex code:  &lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;rDD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paralize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;distFile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;textFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;readme.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;// elements are lines in the file&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;SPARK TRANSFORMATIONS&lt;/h2&gt;
&lt;p&gt;to create new dataset from existing one (lazy)&lt;/p&gt;
&lt;p&gt;examples of transformations: &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image003.png" /&gt;&lt;/p&gt;
&lt;h2&gt;PYTHON LAMBDA FUNCTIONS&lt;/h2&gt;
&lt;p&gt;single expression&lt;/p&gt;
&lt;h2&gt;TRANSFORMATIONS&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image004.png" /&gt;&lt;/p&gt;
&lt;p&gt;⇒ spark truns the function litral into a cloture, balck code runs in driver, green code in workers&lt;/p&gt;
&lt;h2&gt;SPARK ACTIONS&lt;/h2&gt;
&lt;p&gt;cause spark to execute recipe to transform source. 
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image006.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SPARK PROGRAMMING MODEL&lt;/h2&gt;
&lt;h2&gt;CACHING RDDS&lt;/h2&gt;
&lt;p&gt;to avoid having to reload data: &lt;code&gt;rdd.cache()&lt;/code&gt;⇒ read from memory instead of disk&lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image007.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SPARK PROGRAM LIFECYCLE&lt;/h2&gt;
&lt;p&gt;create/paralise ⇒ transform ⇒ [cache] ⇒ action&lt;/p&gt;
&lt;h2&gt;SPARK KEY-VALUE RDDS&lt;/h2&gt;
&lt;p&gt;each element of a &lt;em&gt;pair RDD&lt;/em&gt; is a pair tuple&lt;/p&gt;
&lt;p&gt;key-value transformations: &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image008.png" /&gt;&lt;/p&gt;
&lt;p&gt;ex:&lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image009.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image010.png" /&gt;&lt;/p&gt;
&lt;p&gt;careful using &lt;code&gt;groupByKey&lt;/code&gt;: create lots of data traffic and iterables at works&lt;/p&gt;
&lt;h2&gt;PYSPARK CLOSURES&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;one closure per worker is sent &lt;em&gt;with every task&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;no communication between workers&lt;/li&gt;
&lt;li&gt;changes to global vars will not effect driver / other workers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;⇒ pbs: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inefficient to send large data to each job&lt;/li&gt;
&lt;li&gt;one-way: driver → worker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;pyspark shared vaiables&lt;/strong&gt;: 
2 types: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Broadcase variables&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;send large, read-only variables to all workers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accumulators&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;aggregate values from worker to drivers &lt;/li&gt;
&lt;li&gt;only driver can access its value&lt;/li&gt;
&lt;li&gt;for workers the accumulators are write-only&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;SPARK BROADCAST VARIABLES&lt;/h2&gt;
&lt;p&gt;ex. give every worker a large dataset &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image011.png" /&gt;&lt;/p&gt;
&lt;h2&gt;SPARK ACCUMULATORS&lt;/h2&gt;
&lt;p&gt;can only be "add" to by associative operation &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image012.png" /&gt;&lt;/p&gt;
&lt;p&gt;careful to use accumulators in transformations: &lt;br /&gt;
&lt;img alt="" src="./sparkmooc_note_lec4/pasted_image013.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Lab1&lt;/h2&gt;
&lt;p&gt;VB更新以后虚拟机打不开了, 解决办法在: &lt;br /&gt;
&lt;a href="http://bbs.deepin.org/forum.php?mod=viewthread&amp;amp;tid=26001"&gt;http://bbs.deepin.org/forum.php?mod=viewthread&amp;amp;tid=26001&lt;/a&gt;&lt;/p&gt;</summary><category term="spark"></category></entry></feed>